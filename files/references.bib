@ARTICLE{Lakens2018-ri,
  title = {Equivalence testing for psychological research: A tutorial},
  author = {Lakens, Daniël and Scheel, Anne M and Isager, Peder M},
  journaltitle = {Advances in methods and practices in psychological science},
  publisher = {SAGE Publications},
  volume = {1},
  issue = {2},
  pages = {259-269},
  date = {2018-06-01},
  doi = {10.1177/2515245918770963},
  issn = {2515-2459,2515-2467},
  abstract = {Psychologists must be able to test both for the presence of an
  effect and for the absence of an effect. In addition to testing against zero,
  researchers can use the two one-sided tests (TOST) procedure to test for
  equivalence and reject the presence of a smallest effect size of interest
  (SESOI). The TOST procedure can be used to determine if an observed effect is
  surprisingly small, given that a true effect at least as extreme as the SESOI
  exists. We explain a range of approaches to determine the SESOI in
  psychological science and provide detailed examples of how equivalence tests
  should be performed and reported. Equivalence tests are an important extension
  of the statistical tools psychologists currently use and enable researchers to
  falsify predictions about the presence, and declare the absence, of meaningful
  effects.},
  url = {https://journals.sagepub.com/doi/full/10.1177/2515245918770963},
  urldate = {2023-08-03},
  language = {en}
}

@ARTICLE{Liddell2018-wu,
  title = {Analyzing ordinal data with metric models: What could possibly go
  wrong?},
  author = {Liddell, Torrin M and Kruschke, John K},
  journaltitle = {Journal of experimental social psychology},
  publisher = {Elsevier},
  volume = {79},
  pages = {328-348},
  date = {2018-11-01},
  doi = {10.1016/j.jesp.2018.08.009},
  issn = {0022-1031},
  abstract = {We surveyed all articles in the Journal of Personality and Social
  Psychology (JPSP), Psychological Science (PS), and the Journal of Experimental
  Psychology: General (JEP:G) that mentioned the term “Likert,” and found that
  100\% of the articles that analyzed ordinal data did so using a metric model.
  We present novel evidence that analyzing ordinal data as if they were metric
  can systematically lead to errors. We demonstrate false alarms (i.e.,
  detecting an effect where none exists, Type I errors) and failures to detect
  effects (i.e., loss of power, Type II errors). We demonstrate systematic
  inversions of effects, for which treating ordinal data as metric indicates the
  opposite ordering of means than the true ordering of means. We show the same
  problems — false alarms, misses, and inversions — for interactions in
  factorial designs and for trend analyses in regression. We demonstrate that
  averaging across multiple ordinal measurements does not solve or even
  ameliorate these problems. A central contribution is a graphical explanation
  of how and when the misrepresentations occur. Moreover, we point out that
  there is no sure-fire way to detect these problems by treating the ordinal
  values as metric, and instead we advocate use of ordered-probit models (or
  similar) because they will better describe the data. Finally, although
  frequentist approaches to some ordered-probit models are available, we use
  Bayesian methods because of their flexibility in specifying models and their
  richness and accuracy in providing parameter estimates. An R script is
  provided for running an analysis that compares ordered-probit and metric
  models.},
  url = {https://www.sciencedirect.com/science/article/pii/S0022103117307746},
  file = {Liddell and Kruschke 2018 - Analyzing ordinal data with metric models - What could possibly go wrong.pdf},
  keywords = {Ordinal data; Likert; Ordered-probit; Bayesian analysis;Statistics}
}
